<section data-type="chapter">
<h1><span id="docs-internal-guid-568e7f96-b15c-3fb5-f5e6-7f84d1265eef">Rethinking Database Design with Apache Drill</span></h1>

<p><span id="docs-internal-guid-205ec7e8-b169-9163-bb6f-e3ab36c90a3b">Drill is a SQL-engine for everything (almost). From simple tabular data, to semi-structured to even the most complex structured JSON data. In this two part series we will explore what Apache Drill can do and how it enables us to rethink database design to make everyone's life easier. &nbsp;In this first installment, we will use the National Nutrient Database (NNDB) to illustrate how to explore a new database with Drill. &nbsp;The NNDB is convenient to use and an openly available database, and, when coupled with Drill, you can use it without having to be a software engineer. There is no need to figure out how to transform and load data into a relational database just to query the data. You can even use Drill as a datasource within any application (e.g. Tableau, Microsoft Excel) which can connect to a database through a standard interface. &nbsp;Let’s get started.</span></p>

<section data-type="sect1">
<h1><span id="docs-internal-guid-205ec7e8-b15c-7f72-537a-fe8cccbf1ecf">USDA National Nutrient Database</span></h1>

<p><span id="docs-internal-guid-205ec7e8-b166-163f-6a15-82eb3da02934">The National Nutrient Database (NNDB) is provided by the USDA to allow the public to get standardized information on foods. Documentation for the database and all supporting information on the <a href="http://www.ars.usda.gov/Services/docs.htm?docid=8964">USDA website</a>. Each release of the database and documentation receives a version number. At the time this article is published the current release is referred to as Standard Reference, Release 27 or SR27. The table structures / file definitions do not appear to change dramatically over time. It is quite likely that the queries provided here will continue to work with future releases with little or no modifications.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b166-163f-6a15-82eb3da02934">The entire database can be downloaded in a <a href="http://www.ars.usda.gov/Services/docs.htm?docid=24912">zipped package</a> of ASCII files. In order to follow along in a hands-on manner with the rest of this article / tutorial, you will want to download the zip file and extract the files to /tmp (or equivalent). Drill has a workspace named tmp which references the system’s respected temporary folder. When unzipping the files, ensure they are extracted into a folder named sr27asc. All of the commands below reference the tmp workspace and that folder name (e.g. /tmp/sr27asc).</span></p>

<p><strong><span id="docs-internal-guid-205ec7e8-b166-fc24-2ba3-37ac46ffe919">Starting and Configuring Drill to Handle the NNDB</span></strong></p>

<p><span id="docs-internal-guid-205ec7e8-b166-fc24-2ba3-37ac46ffe919">To start your Drill instance you can use either of these two command lines:</span></p>

<pre data-type="programlisting">
 &gt; sqlline -u jdbc:drill:zk=local

&nbsp;# this is a shortcut for the previous line (linux/mac only)
 &gt; drill-embedded 
</pre>

<p><span id="docs-internal-guid-205ec7e8-b166-fc24-2ba3-37ac46ffe919">First and foremost, it will be important for you to know that you can configure Drill via the web interface. The Drill configuration can be accessed at <a href="http://localhost:8047/">http://localhost:8047/</a> after you start your Drill instance.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b166-fc24-2ba3-37ac46ffe919">The NNDB uses characters that most people don’t use for delimited file formats. The fields are separated by carets (^) and text fields are surrounded by tildes (~). Because of this we need to add a little bit of configuration to enable Drill to properly read / parse these files. Paste this file format definition to the formats section of the dfs storage definition which can be found in Drill’s web interface under the storage menu option:</span></p>

<pre data-code-language="json" data-type="programlisting">
 &nbsp;"nndb": {
&nbsp;&nbsp;&nbsp;&nbsp;"type": "text",
&nbsp;&nbsp;&nbsp;&nbsp;"extensions": [ "txt" ],
&nbsp;&nbsp;&nbsp;&nbsp;"quote": "~",
&nbsp;&nbsp;&nbsp;&nbsp;"escape": "~",
&nbsp;&nbsp;&nbsp;&nbsp;"delimiter": "^"
&nbsp;&nbsp;},
</pre>

<p><span id="docs-internal-guid-205ec7e8-b166-fc24-2ba3-37ac46ffe919">Next, let’s convert these tables into Parquet format to simplify the queries. As we do that, we can also add a workspace definition that has parquet as a default format and allows tables to be created. Workspaces are abbreviations are a way to access files or tables. Paste the definition below into the workspaces section of your dfs storage definition. Make sure that you change the location parameter to the directory where you want to store your tables. </span></p>

<pre data-code-language="json" data-type="programlisting">
&nbsp; "nndb": {
&nbsp;&nbsp;&nbsp;&nbsp;"location": "/opt/drill/nndb",
&nbsp;&nbsp; &nbsp;"writable": true,
&nbsp;&nbsp;&nbsp;&nbsp;"storageformat": "parquet"
&nbsp;&nbsp;},
</pre>
</section>

<section data-type="sect1">
<h1><span id="docs-internal-guid-205ec7e8-b15c-d64a-98eb-2b8eafb516df">Creating Tables from Delimited Files</span></h1>

<p><span id="docs-internal-guid-205ec7e8-b163-b76f-a900-59ea1ccd0a25">For Drill, delimited files are really just tables. They contain rows of data in a columnar form. Delimited files are very simple, but definitely not the most efficient format if you are going to be querying large volumes of data regularly. Delimited files can work great, but if size or speed ever become a concern you will be quickly be looking for alternative storage formats. For this use case, we are going to convert all of the delimited files to Parquet format with Drill.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b163-b76f-a900-59ea1ccd0a25">The queries below do this conversion for each of the data files in the NNDB. Note how the source data columns do not have names and are referenced as columns[N], but are named as part of the create table query. Having named columns makes queries cleaner, easier to read and to understand. The table and column names we use here are taken from the original documentation for this dataset.</span></p>

<pre data-code-language="sql" data-type="programlisting">
-- Referenced from Page 30 of the sr27_doc.pdf
create table dfs.nndb.food_des(NDB_No, FdGrp_Cd, Long_Desc, Shrt_Desc, ComName, ManufacName, Survey, Ref_desc, Refuse, SciName, N_Factor, Pro_Factor, Fat_Factor, CHO_Factor) as select columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], columns[12], columns[13] from dfs.tmp.`sr27asc/FOOD_DES.txt`;

-- Referenced from Page 31 of the sr27_doc.pdf
create table dfs.nndb.fd_group(FdGrp_Cd, FdGrp_Desc) as select columns[0], columns[1] from dfs.tmp.`sr27asc/FD_GROUP.txt`;

-- Referenced from Page 31 of the sr27_doc.pdf
create table dfs.nndb.langual(NDB_No, Factor_Code) as select columns[0], columns[1] from dfs.tmp.`sr27asc/LANGUAL.txt`;

-- Referenced from Page 31-32 of the sr27_doc.pdf
create table dfs.nndb.langdesc(Factor_Code, Description) as select columns[0], columns[1] from dfs.tmp.`sr27asc/LANGDESC.txt`;

-- Referenced from Page 32-33 of the sr27_doc.pdf
create table dfs.nndb.nut_data(NDB_No, Nutr_No, Nutr_Val, Num_Data_Pts, Std_Error, Src_Cd, Deriv_Cd, Ref_NDB_No, Add_Nutr_Mark, Num_Studies, `Min`, `Max`, DF, Low_EB, Up_EB, Stat_cmt, AddMod_Date, CC) as select columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6], columns[7], columns[8], columns[9], columns[10], columns[11], columns[12], columns[13], columns[14], columns[15], columns[16], columns[17] from dfs.tmp.`sr27asc/NUT_DATA.txt`;

-- Referenced from Page 34 of the sr27_doc.pdf
create table dfs.nndb.nutr_def(Nutr_No, Units, Tagname, NutrDesc, Num_Dec, SR_Order) as select columns[0], columns[1], columns[2], columns[3], columns[4], columns[5] from dfs.tmp.`sr27asc/NUTR_DEF.txt`;

-- Referenced from Page 34-35 of the sr27_doc.pdf
create table dfs.nndb.src_cd(Src_Cd, SrcCd_Desc) as select columns[0], columns[1] from dfs.tmp.`sr27asc/SRC_CD.txt`;

-- Referenced from Page 35 of the sr27_doc.pdf
create table dfs.nndb.deriv_cd(Deriv_Cd, Deriv_Desc) as select columns[0], columns[1] from dfs.tmp.`sr27asc/DERIV_CD.txt`;

-- Referenced from Page 36 of the sr27_doc.pdf
create table dfs.nndb.weight(NDB_No, Seq, Amount, Msre_Desc, Gm_Wgt, Num_Data_Pts, Std_Dev) as select columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6] from dfs.tmp.`sr27asc/WEIGHT.txt`;

-- Referenced from Page 36-37 of the sr27_doc.pdf
create table dfs.nndb.footnote(NDB_No, Footnt_No, Footnt_Typ, Nutr_No, Footnt_Txt) as select columns[0], columns[1], columns[2], columns[3], columns[4] from dfs.tmp.`sr27asc/FOOTNOTE.txt`;

-- Referenced from Page 37 of the sr27_doc.pdf
create table dfs.nndb.datsrcln(NDB_No, Nutr_No, DataSrc_ID) as select columns[0], columns[1], columns[2] from dfs.tmp.`sr27asc/DATSRCLN.txt`;

-- Referenced from Page 37-38 of the sr27_doc.pdf
create table dfs.nndb.data_src(DataSrc_ID, Authors, Title, `Year`, Journal, Vol_City, Issue_State, Start_Page, End_Page) as select columns[0], columns[1], columns[2], columns[3], columns[4], columns[5], columns[6], columns[7], columns[8] from dfs.tmp.`sr27asc/DATA_SRC.txt`;
</pre>

<p><span id="docs-internal-guid-205ec7e8-b165-3d41-f161-014f196c5a6b">If you put all these queries in a file called nndb.sql, then you could have Drill run all of these commands at one time:</span></p>

<pre data-type="programlisting">
drill-embedded --run=nndb.sql
</pre>

<p><span id="docs-internal-guid-205ec7e8-b165-aebf-379a-2aca1fae1814">Otherwise, just paste them all into the command line and they will run in order. This should take about 15 seconds. </span></p>
</section>

<section data-type="sect1">
<h1><span>Working with the Data</span></h1>

<p><span id="docs-internal-guid-205ec7e8-b15d-fed1-74b6-e25c258ba3e0">Now that we have the data in a convenient form, we are free to do anything our hearts desire with this dataset. From inside Drill, you can refer to these tables as dfs.nndb.TABLE_NAME. This means that the new database files which contain the data from the original files is now in the directory you associated with the nndb workspace when you configured the nndb workspace.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15e-1341-d050-88ca0b3c1bc4">Here is an entity relationship diagram (ERD) which should make querying this data a little easier.</span></p>

<figure><img alt="" class="inndb-sr27_-_complexpng" src="nndb-sr27_-_Complex.png" />
<figcaption>
<p><span>USDA NNDB SR27 - Standard</span></p>
</figcaption>
</figure>

<p><span id="docs-internal-guid-205ec7e8-b15f-3126-1aab-056178dd041a">When starting to work with a new dataset which I do not fully understand, I tend to explore the data and do what some people call introspection. This is really helpful for getting a feel for the data. When doing this, I commonly limit all of my queries to return only about 20 lines of data. This makes it much easier to get an understanding of the data rather than potentially seeing thousands of lines scrolling by.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15f-3126-1aab-056178dd041a">In this dataset, the first thing to point is that the Food Description (<strong>food_des</strong>) table and the Nutritional Data (<strong>nut_data</strong>) are the hubs for all the data in this database. The complete definitions for all the fields in these tables can be found in the <a href="https://www.ars.usda.gov/SP2UserFiles/Place/12354500/Data/SR27/sr27_doc.pdf">SR27 document</a>.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15f-3126-1aab-056178dd041a">The <strong>langual</strong> table and the <strong>langdesc</strong> table contain the generalized definitions of the items listed in the Food Description table. This query can help you get an idea of the data in those tables:</span></p>

<pre data-type="programlisting">
SELECT lf.NDB_No, ld.Description FROM 
dfs.nndb.langual lf, dfs.nndb.langdesc ld 
where lf.FACTOR_CODE=ld.FACTOR_CODE limit 20;
</pre>

<p><span id="docs-internal-guid-205ec7e8-b15f-c08f-1132-4c4ad3088775">This query could have been written with the JOIN USING syntax:</span></p>

<pre data-code-language="sql" data-type="programlisting">
SELECT lf.NDB_No, ld.Description FROM dfs.nndb.langual lf 
LEFT JOIN dfs.nndb.langdesc ld USING (FACTOR_CODE) limit 20;
</pre>

<p><span id="docs-internal-guid-205ec7e8-b15f-c08f-1132-4c4ad3088775">There are footnotes related to food descriptions and there are footnotes related to nutrition data. These tables can be joined using this query:</span></p>

<pre data-type="programlisting">
SELECT * FROM dfs.nndb.nut_data nd, dfs.nndb.footnote fn 
where nd.ndb_no=fn.ndb_no and nd.nutr_no=fn.nutr_no 
and nd.Nutr_No is not null limit 10;</pre>

<p><span id="docs-internal-guid-205ec7e8-b15f-c08f-1132-4c4ad3088775">The final query we will use will cover joining the core tables to get a better view of the data:</span></p>

<pre data-type="programlisting">
SELECT * FROM
dfs.nndb.food_des fd
LEFT JOIN dfs.nndb.fd_group fg ON (fg.FdGrp_Cd=fd.FdGrp_Cd)
LEFT JOIN dfs.nndb.weight w ON (fd.NDB_No=w.NDB_No)
LEFT JOIN dfs.nndb.nut_data nd ON (fd.NDB_No=nd.NDB_No)
LEFT JOIN dfs.nndb.src_cd sc ON (nd.Src_Cd=sc.Src_Cd)
LEFT JOIN dfs.nndb.nutr_def ndd ON (nd.Nutr_No=ndd.Nutr_No)
LEFT JOIN dfs.nndb.deriv_cd dc ON (nd.Deriv_Cd=dc.Deriv_Cd)
ORDER BY fd.NDB_No ASC limit 20;
</pre>

<p><span id="docs-internal-guid-205ec7e8-b15f-c08f-1132-4c4ad3088775">With this query we can see the data really starts to take shape. It becomes a little easier to see what is in this database when everything is joined together.</span></p>
</section>

<section data-type="sect1">
<h1><span id="docs-internal-guid-205ec7e8-b15d-43e9-7c4b-e0ce410eb599">What’s Next?</span></h1>

<p><span id="docs-internal-guid-205ec7e8-b15d-5866-0d9f-585e6fb023f4">Now that you have an understanding of how to query this database, you can use it to explore the different nutritional information for products you consume or have considered trying. You could calculate the complete nutritional information from a recipe card.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15d-5866-0d9f-585e6fb023f4">The NNDB is convenient to use and an openly available database, and, when coupled with Drill, you can use it without having to be a software engineer. There is no need to figure out how to transform and load data into a relational database just to query the data. You can even use Drill as a datasource within any application (e.g. Tableau, Microsoft Excel) which can connect to a database through a standard interface using JDBC or ODBC connectors.</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15d-5866-0d9f-585e6fb023f4">You don’t have to transform the data in order to use Drill against your datasource. If a data set is too large you can go from running Drill queries on your laptop to running them on your cluster. Drill also supports joining across data sources at the same time (e.g. join csv with parquet with json with hbase).</span></p>

<p><span id="docs-internal-guid-205ec7e8-b15d-5866-0d9f-585e6fb023f4">In part two of this series we will dig deeper into this dataset and look at more complex functionality that is available for use from within Drill. We will also explore how Drill enables us to evolve to a SQL + NoSQL approach.</span></p>

<p>Do you have a great new idea for how to leverage Drill or even this dataset? Please share it below for everyone to read. I would love to hear about them.</p>
</section>
</section>
